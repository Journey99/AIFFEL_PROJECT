{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f802c87c",
   "metadata": {},
   "source": [
    "## < 4. 멋진 작사가 만들기 >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65242746",
   "metadata": {},
   "source": [
    "## [루브릭]\n",
    "1. 가사 텍스트 생성 모델이 정상적으로 동작하는가?\n",
    "2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가?\n",
    "3. 텍스트 생성모델이 안정적으로 학습되었는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c646481",
   "metadata": {},
   "source": [
    "## Step1. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "706b395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f111c63",
   "metadata": {},
   "source": [
    "### glob\n",
    "- glob 모듈의 glob 함수는 사용자가 제시한 조건에 맞는 파일명을 리스트 형식으로 반환\n",
    "- 단, 조건에 정규식을 사용할 수 없으며 엑셀 등에서도 사용할 수 있는 ' * '와 ' ? '같은 와일드카드만을 지원\n",
    "- glob를 활용하여 모든 txt 파일을 읽어온 후, raw_corpus 리스트에 문장 단위로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41e70d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f49535",
   "metadata": {},
   "source": [
    "## Step2. 데이터 정제\n",
    "- 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc1a66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) \n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) \n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) \n",
    "    sentence = sentence.strip() \n",
    "    sentence = '<start> ' + sentence + ' <end>' \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702275b8",
   "metadata": {},
   "source": [
    "### preprocess_sentence()\n",
    "1. 소문자로 바꾸고, 양쪽 공백을 지운다\n",
    "2. 특수문자 양쪽에 공백을 넣고\n",
    "3. 여러개의 공백은 하나의 공백으로 바꾼다\n",
    "4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾼다\n",
    "5. 다시 양쪽 공백을 지운다\n",
    "6. 문장 시작에는 \\<start>, 끝에는 \\<end>를 추가한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d13c09a",
   "metadata": {},
   "source": [
    "### re.sub（정규 표현식, 대상 문자열 , 치환 문자）\n",
    "- 정규 표현식 : 검색 패턴을 지정\n",
    "- 대상 문자열 : 검색 대상이 되는 문자열\n",
    "- 치환 문자 : 변경하고 싶은 문자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc19bef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정제된 문장을 넣을 리스트\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 원하지 않는 문장은 건너뛴다\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    tmp = preprocess_sentence(sentence)\n",
    "\n",
    "    # 길이가 15를 넘어가면 리스트에 넣지 않는다\n",
    "    if len(tmp.split()) > 15 : continue \n",
    "    corpus.append(tmp)\n",
    "    \n",
    "        \n",
    "    \n",
    "# 정제된 결과를 10개만 확인\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee04d954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    4 ...    0    0    0]\n",
      " [   2   15 2967 ...    0    0    0]\n",
      " [   2   33    7 ...   46    3    0]\n",
      " ...\n",
      " [   2    4  118 ...    0    0    0]\n",
      " [   2  258  194 ...   12    3    0]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fa0de15b2b0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    \n",
    "    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꾼다\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) \n",
    "    \n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰준다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef4b76f",
   "metadata": {},
   "source": [
    "### tf.keras.preprocessing.text.Tokenizer( num_words=None, filters, lower=True, split=' ', char_level=False, oov_token=None,document_count=0, **kwargs)\n",
    "- Tokenizer은 사전에 있는 단어의 순서에 맞게 단어를 turning하면서 말뭉치를 벡터화시킨다. 이때 숫자로 변환된 데이터를 텐서(tensor)라고 칭한다\n",
    "    - num_words : 단어 빈도가 많은 순서로 num_words개의 단어를 보존한다\n",
    "    - filters : 걸러낼 문자 리스트를 적어준다. 디폴트는 '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n' 이다\n",
    "    - lower : 입력받은 문자열을 소문자로 변환할지를 True, False로 적어준다\n",
    "    - split : 문자열을 적어 줘야 하고, 단어를 분리하는 기준을 적어준다\n",
    "    - char_level : True인 경우 모든 문자가 토큰으로 처리된다\n",
    "    - ov_token : 값이 지정된 경우, text_to_sequence 호출 과정에서 word_index에 추가되어 out-of-vocabulary words를 대체한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96dd89",
   "metadata": {},
   "source": [
    "### fit_on_texts()\n",
    "- 문자 데이터를 입력받아서 리스트의 형태로 변환한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6b66c",
   "metadata": {},
   "source": [
    "### texts_to_sequences()\n",
    "- 텍스트 안의 단어들을 숫자의 시퀀스의 형태로 변환한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e55ada8",
   "metadata": {},
   "source": [
    "### tf.keras.preprocessing.sequence.pad_sequences()\n",
    "- 시퀀스를 패딩을 하여 리스트의 길이를 동일하게 맞춰준다\n",
    "- 파라미터 :\n",
    "    - sequences: 리스트의 리스트로, 각 성분이 시퀀스\n",
    "    - maxlen: 정수, 모든 시퀀스의 최대 길이를 설정하여 제한한다. 10을 넣으면 10보다 큰 것들은 자른다\n",
    "    - dtype: 출력 시퀀스의 자료형. 가변적 길이의 문자열로 시퀀스를 패딩 하려면object를 사용하면 된다\n",
    "    - padding: 문자열이 들어간다, 'pre'가 디폴트 값으로 앞쪽에 0이 추가되고, 'post'는 뒤쪽으로 0이 추가되어 각 시퀀스를 패딩 한다\n",
    "    - truncating: 문자열, 'pre'는 길이가 초과됐을 때 앞쪽을 자르고 'post'는 maxlen보다 큰 시퀀스의 끝의 값들을 제거한다\n",
    "    - value: 부동소수점 혹은 문자열, 패딩 할 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cae1004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 단어 사전 확인\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef7069c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "src_input = tensor[:, :-1]  \n",
    "\n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062f89c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839f3b52",
   "metadata": {},
   "source": [
    "## Step4. 평가 데이터셋 분리\n",
    "- 단어장의 크기는 12,000 . 총 데이터의 20%를 평가 데이터셋으로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2de623c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "470a8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "741108c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124810, 14)\n",
      "Target Train: (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a5691bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_train)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_train) // BATCH_SIZE\n",
    "\n",
    " # tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad471a90",
   "metadata": {},
   "source": [
    "### tf.data.Dataset\n",
    "- tf.data 는 데이터 입력 파이프 라인 빌드를 위한 텐서플로우의 서브패키지이다. 로컬 파일이나 메모리에 올려져 있는 데이터를 모델에 집어넣기 적합한 텐서로 변환하는 작업을 한다\n",
    "- f.data.dataset 은 tf.data의 추상 클래스로써 데이터의 병렬 처리가 용이한 형태, 즉 GPU가 연산이 끝나면 다음 데이터를 바로바로 가져다가(Pre-Fetch) 빠르게 처리할 수 있도록 고안되었다. \n",
    "    - tf.data.Dataset.from_tensor_slices() : 주어진 데이터소스를 여러 Tensor로 자른다\n",
    "    - dataset.shuffle(buffer_size) : dataset을 섞는다. buffier_size만큼 가져와서 섞는다.\n",
    "    - dataset.batch(BATCH_SIZE, drop_remainder=True) : batch_size를 지정하여 size만큼 데이터를 읽어 들여 학습시킨다. 또한, model이 weight를 업데이트 할 때, 1개의 batch가 끝나고 난 후 업데이트를 하게 되는데, 업데이트 빈도를 조절하는 효과도 있다. drop_remainder는 마지만 남은 데이터를 drop 할 것인지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f89a0b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_val)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(enc_val) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((enc_val, dec_val)).shuffle(BUFFER_SIZE)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4252c38",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f499e7",
   "metadata": {},
   "source": [
    "## Step5. 인공지능 만들기\n",
    "- val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88d268ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 1024\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a4e63",
   "metadata": {},
   "source": [
    "### model\n",
    "- 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성\n",
    "    - Embedding 레이어 :\n",
    "        - 텐서에는 단어 사전의 인덱스가 들어 있는데 이 인덱스 값을 해당 인덱스 번째의 워드 벡터로 바꿔 준다. 이 워드 벡터는 의미 벡터 공간에서 단어의 추상적 표현으로 사용된다. 값이 커질수록 단어의 추상적인 특징들을 더 잡아낼 수 있지만, 그만큼 충분한 데이터가 주어지지 않으면 오히려 혼란만을 야기할 수 있다.\n",
    "    - LSTM 레이어 :\n",
    "        - hidden_size는 모델에 얼마나 많은 일꾼을 둘 것인가?로 생각하면 된다.\n",
    "        - LSTM 레이어는 return_sequence 인자에 따라 마지막 시퀀스에서 한 번만 출력할 수도, 각 시퀀스에서 출력을 할 수도 있다. many to many 문제를 풀거나 LSTM 레이어를 여러 개로 쌓아올릴 때는 return_sequence = True 옵션을 사용한다.\n",
    "        - 참고 : https://sevillabk.github.io/lstm-layer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de40960c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 2.95445934e-04,  1.16711344e-04, -7.31838372e-05, ...,\n",
       "          2.36492604e-04, -4.07708081e-04,  1.13148031e-04],\n",
       "        [ 5.10004640e-04,  1.41668963e-04, -1.85336874e-04, ...,\n",
       "          2.04536656e-04, -9.47582594e-04,  5.78537583e-04],\n",
       "        [ 5.08022145e-04, -3.71952046e-05, -1.33670852e-04, ...,\n",
       "          2.62836606e-04, -9.78183118e-04,  9.86682135e-04],\n",
       "        ...,\n",
       "        [-5.87882358e-04,  3.58387944e-04, -8.79816362e-04, ...,\n",
       "          1.45049766e-03,  2.86493654e-04, -1.10942498e-03],\n",
       "        [-2.31468235e-04,  5.00704336e-04, -1.33500493e-03, ...,\n",
       "          2.09487928e-03,  7.42826494e-04, -9.95192677e-04],\n",
       "        [ 1.18061682e-04,  7.56665366e-04, -1.76946772e-03, ...,\n",
       "          2.71692662e-03,  1.14195689e-03, -8.31042300e-04]],\n",
       "\n",
       "       [[ 2.95445934e-04,  1.16711344e-04, -7.31838372e-05, ...,\n",
       "          2.36492604e-04, -4.07708081e-04,  1.13148031e-04],\n",
       "        [ 1.16597475e-04, -2.25838099e-04,  3.33418429e-04, ...,\n",
       "          3.58526246e-04, -1.23576703e-03,  7.72724336e-04],\n",
       "        [ 1.05253101e-04, -1.18302807e-04,  7.32964254e-05, ...,\n",
       "          1.23295511e-04, -1.45771168e-03,  8.38646782e-04],\n",
       "        ...,\n",
       "        [-3.16310907e-04, -8.68039147e-04,  7.32101034e-04, ...,\n",
       "         -2.43555126e-03, -4.03844635e-04,  2.02029230e-04],\n",
       "        [-4.67915743e-05, -8.07265402e-04,  2.27206459e-04, ...,\n",
       "         -1.79412961e-03,  2.55907769e-04,  1.29830049e-04],\n",
       "        [ 2.62498506e-04, -5.47929609e-04, -2.96852784e-04, ...,\n",
       "         -9.49555077e-04,  8.61355220e-04,  1.04097518e-04]],\n",
       "\n",
       "       [[ 2.95445934e-04,  1.16711344e-04, -7.31838372e-05, ...,\n",
       "          2.36492604e-04, -4.07708081e-04,  1.13148031e-04],\n",
       "        [ 9.29688278e-04, -2.12279803e-04, -2.24294999e-04, ...,\n",
       "          1.32467510e-04, -9.23564585e-05, -3.97837430e-05],\n",
       "        [ 9.89109743e-04, -8.96404963e-04, -7.26434344e-04, ...,\n",
       "          2.52243753e-05, -2.69735872e-04, -1.53029832e-04],\n",
       "        ...,\n",
       "        [-1.80218398e-04, -1.18740590e-03,  2.72385281e-04, ...,\n",
       "         -2.09072325e-03, -8.43788846e-04, -9.19258629e-04],\n",
       "        [-6.77492353e-04, -1.80364901e-03,  1.01560705e-04, ...,\n",
       "         -1.96672487e-03, -6.78111159e-04, -7.57898786e-04],\n",
       "        [-8.88895476e-04, -2.33921921e-03,  3.49976530e-04, ...,\n",
       "         -1.51707151e-03, -5.65655180e-04, -7.28600018e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 2.95445934e-04,  1.16711344e-04, -7.31838372e-05, ...,\n",
       "          2.36492604e-04, -4.07708081e-04,  1.13148031e-04],\n",
       "        [ 9.26047855e-04, -2.13751875e-04, -2.89919524e-04, ...,\n",
       "          2.73811398e-04, -4.20578464e-04,  1.44309510e-04],\n",
       "        [ 1.00963097e-03, -9.40548256e-04, -9.58433200e-04, ...,\n",
       "          1.65724545e-04, -5.96594880e-04,  1.62708326e-04],\n",
       "        ...,\n",
       "        [ 3.59182770e-04,  6.71852787e-04, -1.84841943e-03, ...,\n",
       "          2.06013652e-03,  2.34230864e-03,  4.22819459e-04],\n",
       "        [ 7.39577576e-04,  9.93237481e-04, -2.10622814e-03, ...,\n",
       "          2.75686663e-03,  2.61803623e-03,  5.13083127e-04],\n",
       "        [ 1.00136467e-03,  1.33633323e-03, -2.35337205e-03, ...,\n",
       "          3.41436849e-03,  2.76509114e-03,  5.85073023e-04]],\n",
       "\n",
       "       [[ 2.95445934e-04,  1.16711344e-04, -7.31838372e-05, ...,\n",
       "          2.36492604e-04, -4.07708081e-04,  1.13148031e-04],\n",
       "        [ 8.83079018e-04,  2.52637576e-04, -2.97411432e-04, ...,\n",
       "          1.43759666e-04, -8.30320991e-04,  4.75487759e-04],\n",
       "        [ 9.90475412e-04, -2.25053926e-04, -8.87671777e-04, ...,\n",
       "          3.34495016e-05, -1.33115193e-03,  6.07870461e-04],\n",
       "        ...,\n",
       "        [ 6.65684987e-04,  1.66814227e-03, -8.90023191e-04, ...,\n",
       "          2.24514538e-03,  1.82273134e-03,  2.59722059e-04],\n",
       "        [ 9.02179279e-04,  2.03592260e-03, -1.27490785e-03, ...,\n",
       "          2.93050054e-03,  2.03187717e-03,  2.82924331e-04],\n",
       "        [ 1.01559993e-03,  2.38631666e-03, -1.64552592e-03, ...,\n",
       "          3.56834591e-03,  2.12891330e-03,  3.05121066e-04]],\n",
       "\n",
       "       [[ 2.95445934e-04,  1.16711344e-04, -7.31838372e-05, ...,\n",
       "          2.36492604e-04, -4.07708081e-04,  1.13148031e-04],\n",
       "        [ 4.07953194e-04,  2.39947884e-04, -4.28484600e-05, ...,\n",
       "         -3.10481730e-04, -6.02489628e-04, -1.67588340e-04],\n",
       "        [ 5.87369548e-04, -3.85682943e-04, -4.07084619e-04, ...,\n",
       "         -5.36219450e-04, -8.82117020e-04, -1.27635969e-04],\n",
       "        ...,\n",
       "        [ 3.08663060e-04,  1.08703134e-04, -1.31665124e-03, ...,\n",
       "          2.87520257e-03,  5.74207050e-04,  3.77685938e-04],\n",
       "        [ 4.77103284e-04,  6.05851063e-04, -1.76217093e-03, ...,\n",
       "          3.42678651e-03,  8.98611906e-04,  4.68222192e-04],\n",
       "        [ 5.91516262e-04,  1.11454586e-03, -2.15056376e-03, ...,\n",
       "          3.95964272e-03,  1.13216415e-03,  5.39835368e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7340b4",
   "metadata": {},
   "source": [
    "###  shape=(256, 14, 12001)\n",
    "- 256 : 지정한 배치 사이즈.\n",
    "- 12001 : Dense 레이어의 출력 차원수\n",
    "- 14 : LSTM은 자신에게 입력된 시퀀스의 길이만큼 동일한 길이의 시퀀스를 출력하는데, 데이터셋의 max_len이 14로 맞춰져 있었던 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fccb06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  12289024  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  25174016  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  24590049  \n",
      "=================================================================\n",
      "Total params: 95,615,713\n",
      "Trainable params: 95,615,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5021d173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "487/487 [==============================] - 269s 548ms/step - loss: 3.2262 - val_loss: 2.8631\n",
      "Epoch 2/7\n",
      "487/487 [==============================] - 284s 583ms/step - loss: 2.6801 - val_loss: 2.6108\n",
      "Epoch 3/7\n",
      "487/487 [==============================] - 285s 585ms/step - loss: 2.3554 - val_loss: 2.4350\n",
      "Epoch 4/7\n",
      "487/487 [==============================] - 285s 585ms/step - loss: 2.0407 - val_loss: 2.2989\n",
      "Epoch 5/7\n",
      "487/487 [==============================] - 286s 586ms/step - loss: 1.7454 - val_loss: 2.2091\n",
      "Epoch 6/7\n",
      "487/487 [==============================] - 286s 587ms/step - loss: 1.4875 - val_loss: 2.1486\n",
      "Epoch 7/7\n",
      "487/487 [==============================] - 286s 587ms/step - loss: 1.2810 - val_loss: 2.1261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0c0334580>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "#Loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(dataset, epochs=7, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbc35df",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba18c653",
   "metadata": {},
   "source": [
    "## Step6. 잘 만들어졌는지 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68a7752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "529c075d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i like the flashy type who pass with dykes <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i like\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "713db9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you are the only thing that keeps me goin <end> '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you are\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df8bd8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> he is surrounded by the <unk> <end> '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> he is\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38b7100d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> she is a vacuum <end> '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> she is\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec3f4a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , i love you , i love you <end> '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6581aaff",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e03e0",
   "metadata": {},
   "source": [
    "## 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fea996",
   "metadata": {},
   "source": [
    "### 1. val_loss 를 2.2 이하로 낮추기 위한 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bbb9c5",
   "metadata": {},
   "source": [
    "#### [embedding_size = 256, hidden_size = 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bc97a",
   "metadata": {},
   "source": [
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fl28E5%2FbtrrcGVFlgc%2FogmK8UyUBiKtTT34M0e4nK%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28964cff",
   "metadata": {},
   "source": [
    "#### [embedding_size = 512, hidden_size = 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ff2785",
   "metadata": {},
   "source": [
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbZNJNZ%2FbtrrazQxf2P%2FLYEbHkVZZf06NgQbwxGnH1%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf06b65c",
   "metadata": {},
   "source": [
    "### 2. 토큰화 했을 때 토큰의 개수가 15개를 넘어가는 문장을 학습 데이터에서 제외\n",
    "- 이 프로젝트를 하면서 가장 고민을 많이 한 부분이다. 토큰화하는 과정에서 for문을 써서도 해보았고 tf.keras.preprocessing.sequence.pad_sequences() 인자에 maxlen=15을 넣어서도 해보았다. 결국 토큰화 하기전 corpus에 문장을 넣을 때, sentence의 길이가 15를 넘어가면 corpus에 넣지 않는 방법으로 처리를 해서 프로젝트를 진행했다. 아래의 사진을 보면 maxlen으로 처리했을 때랑 if문으로 처리했을 때랑 개수 차이가 많이나는걸 볼 수 있다. for문을 이용하면 남겨진 문장들은 문장의 끝이 있는 완성된 문장들이지만, maxlen을 이용하면 문장의 중간이 잘려나간 경우도 포함하게 된다. 이 부분에 대해서는 더 많은 생각이 필요할 것 같다!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab3bc29",
   "metadata": {},
   "source": [
    "[maxlen=15]\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcQdZcs%2Fbtrq9WLXgf6%2FwdWVzYtuemxNF29jJkfkDK%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594def1",
   "metadata": {},
   "source": [
    "[if문]\n",
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FrViis%2Fbtrrd7L1B8Y%2F3EKqNt8T3n1D6sTVFzDQ7k%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbcfcdd",
   "metadata": {},
   "source": [
    "### 3. 과대적합\n",
    "- embedding_size = 1024, hidden_size = 2048, epochs=10 으로 했을 때 epochs이 8일 때 val_loss가 다시 증가하는 현상이 발생했다. val_loss가 증가하는건 학습에 좋지 않고 과대적합이 발생했다 생각해서 과대적합이 발생하기 전까지 학습을 시켰다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b9022",
   "metadata": {},
   "source": [
    "![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbowJFB%2Fbtrq7TWwPix%2FCjsCl4KPdVAsMT7tGTekUk%2Fimg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5de8156",
   "metadata": {},
   "source": [
    "### 4. 느낀점\n",
    "- 이번 프로젝트가 NLP를 처음 접하게 된 프로젝트인데 국어를 좋아하지 않아서 그런지 지금까지 했던 프로젝트 중에서 가장 흥미를 느끼지 못한 프로젝트였다. 아직 한번밖에 접해보지 않았기 때문에 NLP에 대한 마음을 접으려고 하진 않지만 좀 더 해봐야 알 것 같다. 아직 optimizer나 loss에 대해 정확히 알지 못해서 이 부분에 대해서 더 많은 공부를 해야할 것 같고 토큰화에 대해서 좀 더 알아보려고 한다. 문장을 생성해보면 처음 생성된 문장은 존재하는 노래 가사가 그대로 나왔다. 그리고 she is a vacuum, 그녀는 진공 이라는 문장이 생성되었는데 이게 맞는 문장인지는 잘 모르겠다:) 그래도 문장이 만들어지는건 신기했다!!   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
